{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda9b9de31226a91",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Vessel Segmentation\n",
    "This notebook is an overview of the basic functions required to train and run the vessel segmentation model. It will cover the following:\n",
    "1. Data preparation\n",
    "2. Training\n",
    "3. Prediction\n",
    "4. Sweeps\n",
    "5. Writing NIFTI files\n",
    "6. Validation\n",
    "7. Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be26c181732e6a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "The script below will take the training and validation images and their masks and split them into patches of a given size, defined by the width.\n",
    "\n",
    "- The training and validation **image paths** are put into lists.\n",
    "    - The masks named \"XXXX_mask.png\" should be in the same folder as each image.\n",
    "- Normalization applies a clahe to improve contrast\n",
    "- The patches are saved in a \"train\" and \"val\" folder in the output directory\n",
    "  <br/> <br/>\n",
    "- The training images are rotated and flipped for data augmentation\n",
    "- The validation patches outside the brain area aren't saved"
   ]
  },
  {
   "cell_type": "code",
   "id": "96c40336-dde6-4b5b-b0a9-7b60be7ec1ed",
   "metadata": {},
   "source": [
    "from liom_toolkit.segmentation.vseg.make_dataset import make_train_val\n",
    "\n",
    "training = ['data/LSFM_dataset/s23/1350.png', 'data/LSFM_dataset/s23/1000.png', 'data/LSFM_dataset/s23/1500.png',\n",
    "            'data/LSFM_dataset/s23/700.png', 'data/LSFM_dataset/s23/800.png', 'data/LSFM_dataset/s23/1200.png',\n",
    "            'data/LSFM_dataset/s23/1110.png', 'data/LSFM_dataset/s23/575.png']\n",
    "validation = ['data/LSFM_dataset/s23/750.png']\n",
    "normalization = True\n",
    "stride = 256\n",
    "width = 256\n",
    "output_dir = 'data/patches'\n",
    "\n",
    "make_train_val(training, validation, normalization, stride, width, output_dir, threshold=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93bab5c4087c00ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Training\n",
    "The script below will train the model on the patches created above. \n",
    "<br/>\n",
    "The train_model function can be configurated by the learning rate, batch size and epochs hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "c772706f-43dd-42c7-94c9-c36f4e86932a",
   "metadata": {},
   "source": "from liom_toolkit.segmentation.vseg.training import train_model",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9f2b896-183e-4e7f-8191-c42e8a8b997f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Set the parameters required for training\n",
    "dataset_dir = \"data/patches\"\n",
    "output = \"data/training\"\n",
    "device = \"cuda\"\n",
    "learning_rate = 0.003673\n",
    "batch_size = 35\n",
    "epochs = 68"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cb68c26-9464-49e5-aaef-a96ce2a83d6f",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "train_model(dataset_dir=dataset_dir, dev=device, output_train=output, learning_rate=learning_rate,\n",
    "            batch_size=batch_size, epochs=epochs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6f3300da5f936b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. Prediction\n",
    "The script below will run the model on the images and save the results.<br/>\n",
    "The model used is the model that was last registered in the Vessel Segmentation registry (this model is tagged @latest)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9dcfabfe883d78f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from liom_toolkit.segmentation.vseg.predict_one import predict_one\n",
    "from liom_toolkit.segmentation.vseg.model import VsegModel\n",
    "\n",
    "device = \"cuda\"\n",
    "# Model load\n",
    "model = VsegModel(pretrained=True, device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54583a13-28ae-481e-bed0-7fcc0d6bf212",
   "metadata": {},
   "source": [
    "### Prediction for one image\n",
    "\n",
    "- The results are saved in the output_path folder\n",
    "- The same normalization as for the training can be applied for the prediction\n",
    "- The results are not great when the images are patched. In the predict_one function, patching can be skipped. Otherwise, if patching is on, a stride and width value have to be given.\n",
    "- The images are currently saved as png's, with the vessel pixels given a 255 value"
   ]
  },
  {
   "cell_type": "code",
   "id": "a274ac0b0f51b4f4",
   "metadata": {},
   "source": [
    "image_path = \"data/LSFM/S24/555nm_slices/500.png\"\n",
    "output_path = \"data/prediction/s24\"\n",
    "normalization = True\n",
    "patching = False\n",
    "stride = None\n",
    "width = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42106dd6ec642237",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Run the model\n",
    "prediction = predict_one(model=model, img_path=image_path, save_path=output_path, norm=normalization, dev=device,\n",
    "                         patching=patching, stride=stride, width=width)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b6ef624-3e34-4def-ad45-3b9bcf53c27a",
   "metadata": {},
   "source": [
    "### Prediction for a folder\n",
    "\n",
    "- The prediction for every image in the directory is saved in the output folder\n",
    "- The same paramaters are used as the previous section"
   ]
  },
  {
   "cell_type": "code",
   "id": "102e8d83-365b-433f-9625-ebc48d95fc4e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dir_path = \"data/LSFM/S24/555nm_slices\"\n",
    "output_path = \"data/LSFM_predictions\"\n",
    "normalization = True\n",
    "patching = False\n",
    "stride = None\n",
    "width = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d005703f-2b6c-48d3-a72c-f0309b57db5a",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "\n",
    "# Run the model\n",
    "for images in tqdm(os.listdir(dir_path)):\n",
    "    start = time.time()\n",
    "    if images != \".ipynb_checkpoints\":\n",
    "        image_path = os.path.join(dir_path, images)\n",
    "        _ = predict_one(model=model, img_path=image_path, save_path=output_path, norm=normalization, dev=device,\n",
    "                        patching=patching, stride=stride, width=width)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "print(f\"Total time:{sum(times)}\")\n",
    "print(f\"Average time:{sum(times) / len(times)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da6ae15a-23e4-4ee1-9850-05d754496bac",
   "metadata": {},
   "source": [
    "## 4. Sweeps\n",
    "Sweeps were done to optimize the training batch size, learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "id": "a961a6ff-7fe7-4cf7-b342-1353836fcff2",
   "metadata": {},
   "source": [
    "# Sweep configuration\n",
    "sweep_config = {'method': 'bayes',\n",
    "                'metric': {\n",
    "                    'name': 'Validation Loss',\n",
    "                    'goal': 'minimize'},\n",
    "                'parameters': {\n",
    "                    'batch_size': {\n",
    "                        'distribution': 'int_uniform',\n",
    "                        'max': 50,\n",
    "                        'min': 10\n",
    "                    },\n",
    "                    'epochs': {\n",
    "                        'distribution': 'int_uniform',\n",
    "                        'max': 75,\n",
    "                        'min': 10\n",
    "                    },\n",
    "                    'learning_rate': {\n",
    "                        'distribution': 'uniform',\n",
    "                        'max': 0.005,\n",
    "                        'min': 5e-6\n",
    "                    },\n",
    "                },\n",
    "                'program': 'training.py'\n",
    "                }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ffad2803-b80c-4056-852b-62dd9b5a36b2",
   "metadata": {},
   "source": [
    "sweep_config2 = {'method': 'bayes',\n",
    "                 'metric': {\n",
    "                     'name': 'Validation Loss',\n",
    "                     'goal': 'minimize'},\n",
    "                 'parameters': {\n",
    "                     'epochs': {\n",
    "                         'distribution': 'int_uniform',\n",
    "                         'max': 75,\n",
    "                         'min': 10\n",
    "                     },\n",
    "                 },\n",
    "                 'program': 'training.py'\n",
    "                 }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91ad6116-e35a-4561-8307-6766d168ea80",
   "metadata": {},
   "source": [
    "# Using the configuration, a sweep ID is created\n",
    "import wandb\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config2, entity=\"liom-lab\", project=\"vseg\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "230064d2-197b-472b-9f4b-b79aad4473b3",
   "metadata": {},
   "source": [
    "# Starts the sweep agent\n",
    "import wandb\n",
    "from liom_toolkit.segmentation.vseg import training\n",
    "\n",
    "sweepid = \"liom-lab/vseg/nmwqptw8\"  #ID printed from the previous cell\n",
    "count = 15  # Number of runs \n",
    "\n",
    "wandb.agent(sweepid, function=training.train_model, count=count)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fba0a815-9a1e-4f34-bf1a-af3c03808126",
   "metadata": {},
   "source": [
    "## 5. Writing NIFTI files\n",
    "The following script assembles all the predictions of one volume (inside a folder) into a nifti file. Since predict_one is used, all the individual segmentations are saved in the designated path. <br/>\n",
    "To do this, every segmentation is added to a list, then stacked as a 3D volume numpy array. The ants library saves it as a NIFTI file"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b8d0049-9f50-4a65-91f8-b1cf37862cbc",
   "metadata": {},
   "source": [
    "from liom_toolkit.segmentation.vseg.predict_one import predict_one\n",
    "from liom_toolkit.segmentation.vseg.model import VsegModel\n",
    "\n",
    "device = \"cuda\"\n",
    "# Model load\n",
    "model = VsegModel(pretrained=True, device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fa72cd5-c397-4def-b3fa-4deb37157db0",
   "metadata": {},
   "source": [
    "import ants\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import natsort"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d3cac02-8e57-45d6-b63a-68f8296ef4b8",
   "metadata": {},
   "source": [
    "# Prediction for every image in the directory\n",
    "folder_dir = \"data/LSFM/S23/555nm_slices\"\n",
    "output_path = \"data/LSFM_predictions\"\n",
    "normalization = True\n",
    "\n",
    "image_list = os.listdir(folder_dir)\n",
    "image_list = natsort.natsorted(image_list)\n",
    "if image_list[-1] == '.ipynb_checkpoints':\n",
    "    image_list.remove('.ipynb_checkpoints')\n",
    "\n",
    "image_3D = []\n",
    "\n",
    "for images in tqdm(image_list):\n",
    "    image_path = f\"{folder_dir}/{images}\"\n",
    "    prediction = predict_one(model=model, img_path=image_path, save_path=output_path, norm=normalization, dev=device,\n",
    "                             patching=False)\n",
    "    image_3D.append(prediction)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e736f43-8b27-457f-8de0-e64ae4551004",
   "metadata": {},
   "source": [
    "volume = np.stack(image_3D)\n",
    "volume = np.transpose(volume, (2, 1, 0))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24ef1e49-19b4-4a03-b4f9-c8e390d4da5b",
   "metadata": {},
   "source": [
    "nifti = ants.from_numpy(volume)\n",
    "nifti.to_file('brainslices.nii')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d800d17f-b57e-47d8-bd0a-a18fc2ba1295",
   "metadata": {},
   "source": [
    "## 6. Validation\n",
    "To test the model on new data.\n",
    "\n",
    "- The  **image paths** are put into a list.\n",
    "    - The masks named \"XXXX_mask.png\" should be in the same folder as each image.\n",
    "    - The images should be in a folder with the volume name\n",
    "- The normalization is on and the patching is off\n",
    "\n",
    "The following metrics are calculated:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Jaccard index\n",
    "- f1 score\n",
    "\n",
    "In the save_path folder, the following files are saved:\n",
    "- The predictions -> \"XXXX_segmented.png\"\n",
    "- A comparison of the prediction and the mask -> \"volume_XXXX_comparison.png\"\n",
    "    - In this image, the pixels are shown as follows:\n",
    "      - TP: white\n",
    "      - TN: black\n",
    "      - FN: blue\n",
    "      - FP: red\n",
    "- A CSV file with the metrics for each image and an average -> \"validationmetrics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "id": "760f04420f0c6469",
   "metadata": {},
   "source": [
    "from liom_toolkit.segmentation.vseg.validation import validate_model\n",
    "\n",
    "images = [\"data/LSFM_dataset/s23/750.png\", \"data/LSFM_dataset/s24/1200.png\"]\n",
    "save_path = \"validation\"\n",
    "device = \"cuda\"\n",
    "\n",
    "validate_model(model=model, img_list=images, save_path=save_path, device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7425f81-2879-4c57-a730-c7655894676a",
   "metadata": {},
   "source": [
    "## 7. Others\n",
    "Other code left over from the VSEG project"
   ]
  },
  {
   "cell_type": "code",
   "id": "37664100-8759-4c1b-b3e1-2b9e540ce5ec",
   "metadata": {},
   "source": [
    "from skimage.morphology import binary_erosion, disk\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage.color import gray2rgb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4623863-1bcf-4bdf-9b39-d7cbb68df6b6",
   "metadata": {},
   "source": [
    "# Erodes a mask by one pixel\n",
    "path = \"data/LSFM_dataset/800_mask.png\"\n",
    "output_path = \"data/LSFM_eroded_dataset/800_mask.png\"\n",
    "mask = imread(path)\n",
    "mask = (mask / mask.max()).astype(np.uint8)\n",
    "\n",
    "erosion_disk = disk(1)\n",
    "\n",
    "image = binary_erosion(mask, footprint=erosion_disk)\n",
    "image = image.astype(np.uint8) * 255\n",
    "imsave(output_path, image, cmap=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49d82b86-cf34-4b15-ab6f-b401e735c26e",
   "metadata": {},
   "source": [
    "# Converts the png values from 0-255 to 0-1\n",
    "path = \"data/LSFM_dataset/800_mask.png\"\n",
    "image = imread(path)\n",
    "image = image / image.max()\n",
    "image = image.astype(np.uint8)\n",
    "imsave(path, image, check_contrast=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdd9a149-ca3d-4d00-9e9d-9d41d2755cfa",
   "metadata": {},
   "source": [
    "# Saves the Clahe of the selected image\n",
    "\n",
    "number = 500\n",
    "volume = \"S24\"\n",
    "image_path = f\"data/LSFM/{volume}/555nm_slices/{number}.png\"\n",
    "output_path = f\"data/clahe/{volume}_{number}_eqhist.png\"\n",
    "\n",
    "image = imread(image_path)\n",
    "image = (image / image.max() * 255).astype(np.uint8)\n",
    "image_clahe = equalize_adapthist(image, kernel_size=10, clip_limit=0.05, nbins=128)\n",
    "image_clahe = gray2rgb(image_clahe)\n",
    "image_clahe = (image_clahe / image_clahe.max() * 255).astype(np.uint8)\n",
    "\n",
    "imsave(output_path, image_clahe, cmap=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7647072-82bd-447e-8262-b0d4fdc155da",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
